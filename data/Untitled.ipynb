{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea872f2d-2e05-4e40-b54d-063179a464c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*****************************************************************************************************************************************        \n",
    "# Prophage annotation : \n",
    "#*****************************************************************************************************************************************        \n",
    "from os import system, listdir, chdir, mkdir\n",
    "from os.path import isdir\n",
    "import os\n",
    "import random                                   \n",
    "path_klebsiella=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "path_phageboost=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/phageboost/phageboost_prediction\"\n",
    "\n",
    "good_strains=open(f\"{path_klebsiella}/panacota_pangenome/panacota_pangenome_list.txt\").read().split(\"\\n\")\n",
    "\n",
    "for specie in os.listdir(path_klebsiella):\n",
    "    if specie[0]==\"k\" and os.path.isdir(f\"{path_klebsiella}/{specie}\")== True:\n",
    "        for strain in random.sample(os.listdir(f\"{path_klebsiella}/{specie}/refseq/bacteria\"), len(os.listdir(f\"{path_klebsiella}/{specie}/refseq/bacteria\"))):\n",
    "            if strain in good_strains :\n",
    "                path_fna=f\"{path_klebsiella}/{specie}/refseq/bacteria/{strain}/prokka_annotation_all/{strain}.fna\"\n",
    "                path_prophage=f\"{path_phageboost}/{strain}\"\n",
    "                try :\n",
    "                    mkdir(path_prophage)\n",
    "                except FileExistsError :\n",
    "                    print(\"The output for phageboost already exists for some reason. We shall continue\")\n",
    "                if len(os.listdir(f\"{path_prophage}\")) == 0:\n",
    "                    system(f\"PhageBoost -f {path_fna} -o {path_prophage}  --threads 4\")\n",
    "                    with open(f\"{path_prophage}/process_done\",\"w\") as outfile:\n",
    "                        outfile.write(\"This strain has been studied\")\n",
    "                    \n",
    "                    \n",
    "# Get the prediction scores\n",
    "# Writting some info files ...\n",
    "from os import system, listdir, chdir, mkdir\n",
    "from os.path import isdir\n",
    "import os\n",
    "\n",
    "path_phageboost=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/phageboost\"\n",
    "\n",
    "with open(f\"{path_phageboost}/score_distribution.phageboost.csv\",\"w\") as outfile:\n",
    "    for strain in os.listdir(f\"{path_phageboost}/phageboost_prediction\") :\n",
    "        if len(os.listdir(f\"{path_phageboost}/phageboost_prediction/{strain}\")) > 2 :\n",
    "            for file in os.listdir(f\"{path_phageboost}/phageboost_prediction/{strain}\"):\n",
    "                if file[0:6]==\"phages\":\n",
    "                    info_file=open(f\"{path_phageboost}/phageboost_prediction/{strain}/{file}\").read().split(\"\\n\")[2:]\n",
    "                    for index_info, info in enumerate(info_file):\n",
    "                        if info :\n",
    "                            score=info.split(\"\\t\")[5]\n",
    "                            outfile.write(f\"{strain},{score}\\n\")\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************************************\n",
    "# The fastANI command :\n",
    "# *******************************************************************************************************************************************\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_phageboot_info=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_decipher/phageboost/phageboost_info\"\n",
    "path_phageboost_pred=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_decipher/phageboost/phageboost_prediction\"\n",
    "path_fastANI_2=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "\n",
    "    # But first, the strain_ktype dictionary :\n",
    "strain_ktype={}\n",
    "good_strain=open(f\"{path_ktype}/results_kleborate_count.tsv\").read().split(\"\\n\")\n",
    "for index_strain, info in enumerate(good_strain):\n",
    "    if info:\n",
    "        strain=info.split(\"\\t\")[0].strip()\n",
    "        ktype=info.split(\"\\t\")[2].strip()\n",
    "        strain_ktype[strain]=ktype\n",
    "\n",
    "with open(f\"{path_phageboot_info}/results_phageboost.70.20102022.tsv\",\"w\") as outfile1 :\n",
    "    outfile1.write(f\"Prophage_name\\tProphage_length\\tN_genes\\tScore\\tK_type\\n\")\n",
    "    for strain in tqdm(os.listdir(path_phageboost_pred)):\n",
    "        # Opening the resume file of phageboost prediction :\n",
    "        for file in os.listdir(f\"{path_phageboost_pred}/{strain}\"):\n",
    "            if file[0:6]==\"phages\":\n",
    "                try :\n",
    "                    resume= pd.read_csv(f\"{path_phageboost_pred}/{strain}/{file}\", skiprows=1, sep=\"\\t\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Seems like there is no prophage for \")\n",
    "                #Scanning the file for phage with a score > 0.70\n",
    "                for index_info, info in resume.iterrows():\n",
    "                    if float(info[\"score\"])>= 0.70 :\n",
    "                        # Getting the prophage info :\n",
    "                        prophage_id= info[\"attributes\"].split(\"phage_id=\")[1]\n",
    "                        prophage_len= int(info[\"start\"]) -int(info[\"end\"])\n",
    "                        n_genes= info[\"attributes\"].split(\"n_genes=\")[1].split(\";\")[0]\n",
    "                        for file2 in os.listdir(f\"{path_phageboost_pred}/{strain}\"):\n",
    "                            if file2.count(prophage_id)>0:\n",
    "                                seq=open(f\"{path_phageboost_pred}/{strain}/{file2}\").read().split(\"\\n\")[1]\n",
    "                                if os.path.isfile(f\"{path_fastANI_2}/{strain}__{prophage_id}.fasta\")==False:\n",
    "                                    with open(f\"{path_fastANI_2}/{strain}__{prophage_id}.fasta\",\"w\") as outfile :\n",
    "                                        outfile.write(f\">{strain}__{prophage_id}\\n{seq}\")\n",
    "                        outfile1.write(f\"{strain}__{prophage_id}\\t{str(prophage_len)}\\t{n_genes}\\t{info['score']}\\t{strain_ktype[strain]}\\n\")\n",
    "\n",
    "\n",
    "# Step 2 :\n",
    "# Writting the path file :\n",
    "\n",
    "with open(f\"{path_phageboot_info}/fastANI_list.20102022.tsv\",\"w\") as outfile :\n",
    "    for file in tqdm(os.listdir(path_fastANI_2)):\n",
    "        outfile.write(f\"{path_fastANI_2}/{file}\\n\")\n",
    "        \n",
    "        \n",
    "#*****************************************************************************************************************************************    \n",
    "# Step 3 :        \n",
    "# fatANI commands : \n",
    "\n",
    "#!/bin/bash\n",
    "#BATCH --job-name=fatANI_phb\n",
    "#SBATCH --partition=medium \n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=40\n",
    "#SBATCH --mem=200gb \n",
    "#SBATCH --time=4-00:00:00 \n",
    "#SBATCH --output=fatANI_phb%j.log \n",
    "\n",
    "module restore la_base\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate fastani\n",
    "\n",
    "fastANI  --ql /home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_decipher/phageboost/phageboost_info/fastANI_list.20102022.tsv --rl /home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_decipher/phageboost/phageboost_info/fastANI_list.20102022.tsv -o /home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_out_20102022  --matrix  -t 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40875b4e-2edb-4f93-b15e-1ad8cb753a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************************************************************************************************************************************\n",
    "# Inspecting the fastANI outputs : Approach 2 (20/04/2023)\n",
    "\n",
    "#First round inspection : Get the pairs of prophages with a ANI score>0.99 and coverage > 80%\n",
    "\n",
    "# Output format. In all above use cases, OUTPUT_FILE will contain tab delimited row(s) with query genome, reference genome, \n",
    "# ANI value, count of bidirectional fragment mappings, and total query fragments. \n",
    "# Alignment fraction (wrt. the query genome) is simply the ratio of mappings and total fragments. (https://github.com/ParBLiSS/FastANI)\n",
    "# *******************************************************************************************************************************************\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "path_fastani=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022_out\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "# Generating the dico with the k_type info for each strain\n",
    "strain_ktype={}\n",
    "good_strain=open(f\"{path_ktype}/results_kleborate_count.tsv\").read().split(\"\\n\")\n",
    "for index_strain, info in enumerate(good_strain):\n",
    "    if info:\n",
    "        strain=info.split(\"\\t\")[0].strip()\n",
    "        ktype=info.split(\"\\t\")[2].strip()\n",
    "        strain_ktype[strain]=ktype\n",
    "    \n",
    "fastani_names = [\"Query\",\"Reference_genome\",\"ANI\",\"fragments\",\"total_fragments\"]\n",
    "fastani_df = pd.read_csv(f\"{path_fastani}/fastANI_out_20102022\",sep=\"\\t\", names = fastani_names , nrows = 1000)\n",
    "fastani_df = fastani_df[fastani_df[\"ANI\"] >= 90]\n",
    "\n",
    "families = []\n",
    "fastani_dict = fastani_df.to_dict('records')\n",
    "for row in tqdm(fastani_dict) :\n",
    "    if float(row[\"ANI\"]) >=99 and float(row[\"fragments\"])/float(row[\"total_fragments\"])>=0.80:\n",
    "        l_query = len(open(f\"{row['Query']}\").read().split(\"\\n\")[1]) \n",
    "        l_refer = len(open(f\"{row['Reference_genome']}\").read().split(\"\\n\")[1])\n",
    "        case_1 = (l_query > l_refer and l_query /l_refer >= 0.8)\n",
    "        case_2 = (l_refer > l_query and l_refer /l_query >= 0.8)\n",
    "        #if l_query /l_refer >= 0.8 and l_query /l_refer <= 1.25 : \n",
    "        if case_1 or case_2 :\n",
    "        #if l_query /l_refer >= 0.8 and l_query /l_refer <= 1.25 : \n",
    "            prophage_1 = row[\"Query\"].split(\"/\")[-1]\n",
    "            prophage_2 = row[\"Reference_genome\"].split(\"/\")[-1]\n",
    "            pair = {prophage_1, prophage_2}\n",
    "            for cluster in families :\n",
    "                if cluster.isdisjoint(pair) == False :\n",
    "                    cluster.update(pair)\n",
    "                    break\n",
    "            else :\n",
    "                families.append(pair)\n",
    "\n",
    "\n",
    "with open(f\"{path_fastani}/clusters_99_80.info.2004.v2.tsv\",'w') as outfile :\n",
    "    with open(f\"{path_fastani}/clusters_99_80.2004.v2.tsv\",'w') as outfile_cluster :\n",
    "        outfile.write(\"Family_index\\tMember\\n\")\n",
    "        outfile_cluster.write(\"Family_index\\tMembers\\n\")\n",
    "        for index_c, cluster in enumerate(families) :\n",
    "            outfile_cluster.write(f\"{index_c}\\t\")\n",
    "            cluster_c_l = []\n",
    "            for member in cluster :\n",
    "                outfile.write(f\"family {index_c}\\t{member}\\n\")\n",
    "                cluster_c_l.append(member)\n",
    "            outfile_cluster.write(\",\".join(cluster_c_l))\n",
    "            outfile_cluster.write(\"\\n\")\n",
    "\n",
    "# *******************************************************************************************************************************************                    \n",
    "#!/bin/bash\n",
    "#BATCH --job-name=post_ANI2_\n",
    "#SBATCH --qos=short \n",
    "#SBATCH --ntasks=1 \n",
    "#SBATCH --cpus-per-task=10\n",
    "#SBATCH --mem=10gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=post_ANI2_%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate ScaleAP\n",
    "\n",
    "python3 /home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/script_files/py_files/cluster.I.2004.py\n",
    "# *******************************************************************************************************************************************              \n",
    "# Check the integrety of the DF :\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "path_fastani=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022_out\"\n",
    "path_phages = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "families = pd.read_csv(f\"{path_fastani}/clusters_99_80.2004.v2.tsv\", header = 0, sep='\\t')\n",
    "families_set = [set(fam.split(\",\")) for fam in families[\"Members\"]]\n",
    "\n",
    "n_iteration = 7\n",
    "clean_families = []\n",
    "tmp_families = families_set.copy()\n",
    "# fill up the tmp clusters\n",
    "for n in range(n_iteration):\n",
    "    for index_set, cluster in tqdm(enumerate(tmp_families)) :\n",
    "        for index_2, cluster_2 in enumerate(families_set):\n",
    "            if cluster.isdisjoint(cluster_2) == False :\n",
    "                cluster.update(cluster_2)\n",
    "                continue\n",
    "            else :\n",
    "                continue\n",
    "\n",
    "# Gather the clean clusters :\n",
    "for index, cluster in enumerate(tmp_families) :\n",
    "    if cluster not in clean_families :\n",
    "        clean_families.append(cluster)\n",
    "# *******************************************************************************************************************************************************************\n",
    "# Write the final files :\n",
    "with open(f\"{path_fastani}/clusters_99_80.clean.2004.v2.tsv\",\"w\") as outfile :\n",
    "    outfile.write(\"Family_index\\tMembers\\n\")\n",
    "    phages = set()\n",
    "    L = len(clean_families)\n",
    "    loners = []\n",
    "    for index_f, family in tqdm(enumerate(clean_families_2)) :\n",
    "        cluster_list = \",\".join(list(family))\n",
    "        outfile.write(f\"Family_{index_f}\\t{cluster_list}\\n\")\n",
    "    for phage in tqdm(os.listdir(path_phages)):\n",
    "        for index_f, family in enumerate(clean_families_2):\n",
    "            if phage in family :\n",
    "                break\n",
    "        else :\n",
    "            loners.append(phage)\n",
    "    for index, phage in enumerate(loners) :        \n",
    "        outfile.write(f\"Loner_{str(L+index)}\\t{phage}\\n\")    \n",
    "\n",
    "# *******************************************************************************************************************************************************************\n",
    "# Check the integrity of the files :\n",
    "cluster = pd.read_csv(f\"{path_fastani}/clusters_99_80.clean.v2.tsv\", header = 0, sep=\"\\t\")\n",
    "phages = []\n",
    "cluster_dict = cluster.to_dict(\"records\")\n",
    "\n",
    "for row in tqdm(cluster_dict) :\n",
    "    for member in row[\"Members\"].split(\",\") :\n",
    "        phages.append(member)\n",
    "        \n",
    "loners_df = cluster[cluster[\"Family_index\"]==\"Loner\"]\n",
    "fammmm_df = cluster[cluster[\"Family_index\"]!=\"Loner\"]\n",
    "\n",
    "cluster = pd.read_csv(f\"{path_fastani}/clusters_99_80.clean.2004.v2.tsv\", header = 0, sep=\"\\t\")\n",
    "\n",
    "\n",
    "with open(f\"{path_fastani}/clusters_99_80.extra_clean.2004.v2.tsv\",\"w\") as outfile :\n",
    "    outfile.write(f\"prophage_id\\tprophage\\n\")\n",
    "    for index,row in tqdm(cluster.iterrows()) :\n",
    "        for member in row[\"Members\"].split(\",\") :\n",
    "            outfile.write(f\"prophage_{index}\\t{member}\\n\")\n",
    "            \n",
    "            \n",
    "# Fix the families\n",
    "path_fastani=\"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022_out\"\n",
    "path_phages = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "path_ktype=\"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "families = pd.read_csv(f\"{path_fastani}/clusters_99_80.tsv\", header = 0, sep='\\t')\n",
    "families_set = [set(fam.split(\",\")) for fam in families[\"Members\"]]\n",
    "\n",
    "n_iteration = 10\n",
    "clean_families = []\n",
    "tmp_families = families_set.copy()\n",
    "# fill up the tmp clusters\n",
    "for n in range(n_iteration):\n",
    "    for index_set, cluster in tqdm(enumerate(tmp_families)) :\n",
    "        for index_2, cluster_2 in enumerate(families_set):\n",
    "            if cluster.isdisjoint(cluster_2) == False :\n",
    "                cluster.update(cluster_2)\n",
    "                continue\n",
    "            else :\n",
    "                continue\n",
    "    print(f\"Iteration number {n}\")\n",
    "\n",
    "# Gather the clean clusters :\n",
    "for index, cluster in enumerate(tmp_families) :\n",
    "    if cluster not in clean_families :\n",
    "        clean_families.append(cluster)\n",
    "        \n",
    "        \n",
    "        \n",
    "clean_families = []\n",
    "for index_set, cluster in tqdm(enumerate(families_set)) :\n",
    "    clean_cluster = cluster.copy()\n",
    "    #print(clean_cluster)\n",
    "    for index_2, cluster_2 in enumerate(families_set):\n",
    "        if clean_cluster.isdisjoint(cluster_2) == False :\n",
    "            clean_cluster.update(cluster_2)\n",
    "            continue\n",
    "        else :\n",
    "            continue\n",
    "    #print(clean_cluster)\n",
    "    if clean_cluster not in clean_families :\n",
    "        clean_families.append(clean_cluster)\n",
    "        \n",
    "\n",
    "# Repeat the iteration : \n",
    "clean_families_2 = []\n",
    "for index_set, cluster in tqdm(enumerate(clean_families)) :\n",
    "    clean_cluster = cluster.copy()\n",
    "    for index_2, cluster_2 in enumerate(clean_families):\n",
    "        if clean_cluster.isdisjoint(cluster_2) == False :\n",
    "            clean_cluster.update(cluster_2)\n",
    "            continue\n",
    "        else :\n",
    "            continue\n",
    "    if clean_cluster not in clean_families_2 :\n",
    "        clean_families_2.append(clean_cluster)\n",
    "\n",
    "# *******************************************************************************************************************************************************************\n",
    "n_iteration = 10\n",
    "clean_families = []\n",
    "for n in range(n_iteration):\n",
    "    tmp_families = []\n",
    "    for index_set, cluster in tqdm(enumerate(clean_families)) :\n",
    "        clean_cluster = cluster.copy()\n",
    "        #print(clean_cluster)\n",
    "        for index_2, cluster_2 in enumerate(families_set):\n",
    "            if clean_cluster.isdisjoint(cluster_2) == False :\n",
    "                clean_cluster.update(cluster_2)\n",
    "                continue\n",
    "            else :\n",
    "                continue\n",
    "        #print(clean_cluster)\n",
    "        if clean_cluster not in clean_families :\n",
    "            clean_families.append(clean_cluster)\n",
    "    \n",
    "    \n",
    "\n",
    "# *******************************************************************************************************************************************************************\n",
    "with open(f\"{path_fastani}/clusters_99_80.clean.tsv\",\"w\") as outfile :\n",
    "    outfile.write(\"Family_index\\tMembers\\n\")\n",
    "    phages = set()\n",
    "    L = len(clean_families)\n",
    "    loners = []\n",
    "    for index_f, family in tqdm(enumerate(clean_families_2)) :\n",
    "        cluster_list = \",\".join(list(family))\n",
    "        outfile.write(f\"Family_{index_f}\\t{cluster_list}\\n\")\n",
    "    for phage in tqdm(os.listdir(path_phages)):\n",
    "        for index_f, family in enumerate(clean_families_2):\n",
    "            if phage in family :\n",
    "                break\n",
    "        else :\n",
    "            loners.append(phage)\n",
    "    for index, phage in enumerate(loners) :        \n",
    "        outfile.write(f\"Loner_{str(L+index)}\\t{phage}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_geometric]",
   "language": "python",
   "name": "conda-env-torch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
