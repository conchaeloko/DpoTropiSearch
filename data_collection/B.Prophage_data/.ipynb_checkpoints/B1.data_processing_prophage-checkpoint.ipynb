{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c33ff5-649f-4a34-9fc6-7e438d7c891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "# Prophage work\n",
    "\n",
    "### (I) Predict prophages and call FastANI\n",
    "### (II) Inspecting the output\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726778c-c192-4e60-974a-fbbd60d3d0b4",
   "metadata": {},
   "source": [
    "> Running Phageboost and FastANI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea545b-e555-447b-805d-2be4fbbbf5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_phageboost(path_klebsiella, path_phageboost, good_strains_file, threads=4):\n",
    "    \"\"\"Run PhageBoost on all good strains to predict prophages.\"\"\"\n",
    "    good_strains = open(good_strains_file).read().split(\"\\n\")\n",
    "    \n",
    "    for specie in os.listdir(path_klebsiella):\n",
    "        if specie.startswith(\"k\") and os.path.isdir(f\"{path_klebsiella}/{specie}\"):\n",
    "            strains = os.listdir(f\"{path_klebsiella}/{specie}/refseq/bacteria\")\n",
    "            for strain in random.sample(strains, len(strains)):\n",
    "                if strain in good_strains:\n",
    "                    path_fna = f\"{path_klebsiella}/{specie}/refseq/bacteria/{strain}/prokka_annotation_all/{strain}.fna\"\n",
    "                    path_prophage = f\"{path_phageboost}/{strain}\"\n",
    "                    \n",
    "                    try:\n",
    "                        os.mkdir(path_prophage)\n",
    "                    except FileExistsError:\n",
    "                        print(f\"Output for {strain} already exists. Continuing...\")\n",
    "                    \n",
    "                    if len(os.listdir(path_prophage)) == 0:\n",
    "                        # Run PhageBoost\n",
    "                        cmd = f\"PhageBoost -f {path_fna} -o {path_prophage} --threads {threads}\"\n",
    "                        subprocess.run(cmd, shell=True)\n",
    "                        \n",
    "                        with open(f\"{path_prophage}/process_done\", \"w\") as outfile:\n",
    "                            outfile.write(\"This strain has been studied\")\n",
    "                    else:\n",
    "                        print(f\"PhageBoost already completed for {strain}\")\n",
    "                        \n",
    "\n",
    "def extract_phageboost_scores(path_phageboost, output_score_file):\n",
    "    \"\"\"Extract prophage prediction scores from PhageBoost results.\"\"\"\n",
    "    with open(output_score_file, \"w\") as outfile:\n",
    "        for strain in os.listdir(f\"{path_phageboost}/phageboost_prediction\"):\n",
    "            if len(os.listdir(f\"{path_phageboost}/phageboost_prediction/{strain}\")) > 2:\n",
    "                for file in os.listdir(f\"{path_phageboost}/phageboost_prediction/{strain}\"):\n",
    "                    if file.startswith(\"phages\"):\n",
    "                        info_file = open(f\"{path_phageboost}/phageboost_prediction/{strain}/{file}\").read().split(\"\\n\")[2:]\n",
    "                        for info in info_file:\n",
    "                            if info:\n",
    "                                score = info.split(\"\\t\")[5]\n",
    "                                outfile.write(f\"{strain},{score}\\n\")\n",
    "                                \n",
    "                                \n",
    "def prepare_fastani_input(path_phageboost_pred, path_fastANI, path_phageboot_info, strain_ktype_file, min_score=0.70):\n",
    "    \"\"\"Prepare input for FastANI based on prophage predictions with scores >= min_score.\"\"\"\n",
    "    # Read strain K-type data\n",
    "    strain_ktype = {}\n",
    "    good_strain = open(strain_ktype_file).read().split(\"\\n\")\n",
    "    for info in good_strain:\n",
    "        if info:\n",
    "            strain = info.split(\"\\t\")[0].strip()\n",
    "            ktype = info.split(\"\\t\")[2].strip()\n",
    "            strain_ktype[strain] = ktype\n",
    "    \n",
    "    # Write the prophage information\n",
    "    with open(f\"{path_phageboot_info}/results_phageboost.{min_score}.tsv\", \"w\") as outfile1:\n",
    "        outfile1.write(\"Prophage_name\\tProphage_length\\tN_genes\\tScore\\tK_type\\n\")\n",
    "        \n",
    "        for strain in tqdm(os.listdir(path_phageboost_pred)):\n",
    "            for file in os.listdir(f\"{path_phageboost_pred}/{strain}\"):\n",
    "                if file.startswith(\"phages\"):\n",
    "                    try:\n",
    "                        resume = pd.read_csv(f\"{path_phageboost_pred}/{strain}/{file}\", skiprows=1, sep=\"\\t\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"No prophage data for {strain}\")\n",
    "                        continue\n",
    "                    \n",
    "                    for _, info in resume.iterrows():\n",
    "                        if float(info[\"score\"]) >= min_score:\n",
    "                            prophage_id = info[\"attributes\"].split(\"phage_id=\")[1]\n",
    "                            prophage_len = int(info[\"end\"]) - int(info[\"start\"])\n",
    "                            n_genes = info[\"attributes\"].split(\"n_genes=\")[1].split(\";\")[0]\n",
    "                            \n",
    "                            for file2 in os.listdir(f\"{path_phageboost_pred}/{strain}\"):\n",
    "                                if prophage_id in file2:\n",
    "                                    seq = open(f\"{path_phageboost_pred}/{strain}/{file2}\").read().split(\"\\n\")[1]\n",
    "                                    if not os.path.isfile(f\"{path_fastANI}/{strain}__{prophage_id}.fasta\"):\n",
    "                                        with open(f\"{path_fastANI}/{strain}__{prophage_id}.fasta\", \"w\") as outfile:\n",
    "                                            outfile.write(f\">{strain}__{prophage_id}\\n{seq}\")\n",
    "                                    \n",
    "                            outfile1.write(f\"{strain}__{prophage_id}\\t{prophage_len}\\t{n_genes}\\t{info['score']}\\t{strain_ktype[strain]}\\n\")\n",
    "\n",
    "                            \n",
    "def write_fastani_list(path_fastANI, output_list_file):\n",
    "    \"\"\"Generate the FastANI list of sequences to be used for comparison.\"\"\"\n",
    "    with open(output_list_file, \"w\") as outfile:\n",
    "        for file in tqdm(os.listdir(path_fastANI)):\n",
    "            outfile.write(f\"{path_fastANI}/{file}\\n\")\n",
    "            \n",
    "            \n",
    "def run_fastani(ql_list, rl_list, output_file, threads=40):\n",
    "    \"\"\"Run FastANI using the provided query and reference lists.\"\"\"\n",
    "    fastani_cmd = (\n",
    "        f\"fastANI --ql {ql_list} --rl {rl_list} -o {output_file} --matrix -t {threads}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(fastani_cmd, shell=True, check=True)\n",
    "        print(\"FastANI completed successfully!\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error while running FastANI: {e}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "def main():\n",
    "    # Define paths and files\n",
    "    path_klebsiella = \"/home/conchae/prediction_depolymerase_tropism\"\n",
    "    path_phageboost = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/phageboost/phageboost_prediction\"\n",
    "    good_strains_file = f\"{path_klebsiella}/panacota_pangenome/panacota_pangenome_list.txt\"\n",
    "    output_score_file = f\"{path_phageboost}/score_distribution.phageboost.csv\"\n",
    "    path_fastANI = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "    path_phageboot_info = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_decipher/phageboost/phageboost_info\"\n",
    "    strain_ktype_file = f\"{path_klebsiella}/results_kleborate_count.tsv\"\n",
    "    fastani_list_file = f\"{path_phageboot_info}/fastANI_list.20102022.tsv\"\n",
    "    fastani_output_file = f\"{path_klebsiella}/prophage_prediction/prophage_similarity/phageboost/fastANI_out_20102022\"\n",
    "\n",
    "    # Step 1: Prophage Annotation\n",
    "    print(\"Starting prophage annotation with PhageBoost...\")\n",
    "    run_phageboost(path_klebsiella, path_phageboost, good_strains_file)\n",
    "\n",
    "    # Step 2: Extract Prediction Scores\n",
    "    print(\"Extracting PhageBoost prediction scores...\")\n",
    "    extract_phageboost_scores(path_phageboost, output_score_file)\n",
    "\n",
    "    # Step 3: Prepare FastANI input files\n",
    "    print(\"Preparing FastANI input files...\")\n",
    "    prepare_fastani_input(path_phageboost, path_fastANI, path_phageboot_info, strain_ktype_file)\n",
    "\n",
    "    # Step 4: Write FastANI list\n",
    "    print(\"Writing FastANI list...\")\n",
    "    write_fastani_list(path_fastANI, fastani_list_file)\n",
    "\n",
    "    # Step 5: Run FastANI\n",
    "    print(\"Running FastANI...\")\n",
    "    run_fastani(fastani_list_file, fastani_list_file, fastani_output_file)\n",
    "\n",
    "    print(\"Pipeline complete.\")\n",
    "\n",
    "# Execute main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab84ace-0fd5-445b-aaa3-18b8ac0b05ad",
   "metadata": {},
   "source": [
    "> Inspecting the FastANI output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fb50a-0fb4-42a3-84c1-879dca503d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths to data\n",
    "path_fastani = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022_out\"\n",
    "path_phages = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/prophage_similarity/phageboost/fastANI_20102022\"\n",
    "path_ktype = \"/home/conchae/prediction_depolymerase_tropism\"\n",
    "\n",
    "# Load the fastANI results and filter for ANI ≥ 90\n",
    "fastani_columns = [\"Query\", \"Reference_genome\", \"ANI\", \"fragments\", \"total_fragments\"]\n",
    "fastani_df = pd.read_csv(f\"{path_fastani}/fastANI_out_20102022\", sep=\"\\t\", names=fastani_columns)\n",
    "fastani_df = fastani_df[fastani_df[\"ANI\"] >= 90]\n",
    "\n",
    "# Initialize families\n",
    "families = []\n",
    "fastani_dict = fastani_df.to_dict('records')\n",
    "\n",
    "# Build families of related prophages based on ANI ≥ 99 and coverage ≥ 80%\n",
    "for row in tqdm(fastani_dict):\n",
    "    ani = float(row[\"ANI\"])\n",
    "    coverage = float(row[\"fragments\"]) / float(row[\"total_fragments\"])\n",
    "\n",
    "    # Check the ANI and coverage thresholds\n",
    "    if ani >= 99 and coverage >= 0.80:\n",
    "        # Get lengths of query and reference genomes\n",
    "        l_query = len(open(row['Query']).read().split(\"\\n\")[1])\n",
    "        l_refer = len(open(row['Reference_genome']).read().split(\"\\n\")[1])\n",
    "\n",
    "        # Compare lengths and apply length threshold (within 20%)\n",
    "        length_check = min(l_query, l_refer) / max(l_query, l_refer) >= 0.80\n",
    "\n",
    "        if length_check:\n",
    "            # Extract prophage IDs from paths\n",
    "            prophage_1 = row[\"Query\"].split(\"/\")[-1]\n",
    "            prophage_2 = row[\"Reference_genome\"].split(\"/\")[-1]\n",
    "            pair = {prophage_1, prophage_2}\n",
    "\n",
    "            # Merge clusters if they share prophages\n",
    "            for cluster in families:\n",
    "                if not cluster.isdisjoint(pair):\n",
    "                    cluster.update(pair)\n",
    "                    break\n",
    "            else:\n",
    "                families.append(pair)\n",
    "\n",
    "# Save family clusters to files\n",
    "output_clusters = f\"{path_fastani}/clusters_99_80.info.2004.v2.tsv\"\n",
    "output_members = f\"{path_fastani}/clusters_99_80.2004.v2.tsv\"\n",
    "\n",
    "with open(output_clusters, 'w') as outfile_cluster, open(output_members, 'w') as outfile_member:\n",
    "    outfile_cluster.write(\"Family_index\\tMember\\n\")\n",
    "    outfile_member.write(\"Family_index\\tMembers\\n\")\n",
    "\n",
    "    for index_c, cluster in enumerate(families):\n",
    "        outfile_member.write(f\"{index_c}\\t{','.join(cluster)}\\n\")\n",
    "        for member in cluster:\n",
    "            outfile_cluster.write(f\"Family {index_c}\\t{member}\\n\")\n",
    "\n",
    "# Check integrity of cluster assignments\n",
    "families_df = pd.read_csv(output_members, sep=\"\\t\")\n",
    "families_set = [set(fam.split(\",\")) for fam in families_df[\"Members\"]]\n",
    "\n",
    "# Iterate over families to merge any overlapping clusters (n_iterations = 7)\n",
    "n_iterations = 7\n",
    "for _ in range(n_iterations):\n",
    "    updated_families = []\n",
    "    merged = [False] * len(families_set)\n",
    "\n",
    "    for i, cluster in tqdm(enumerate(families_set)):\n",
    "        if merged[i]:\n",
    "            continue\n",
    "        for j in range(i + 1, len(families_set)):\n",
    "            if cluster.isdisjoint(families_set[j]) == False:\n",
    "                cluster.update(families_set[j])\n",
    "                merged[j] = True\n",
    "\n",
    "        updated_families.append(cluster)\n",
    "\n",
    "    families_set = updated_families\n",
    "\n",
    "# Write the final cleaned clusters to file\n",
    "cleaned_output = f\"{path_fastani}/clusters_99_80.clean.2004.v2.tsv\"\n",
    "with open(cleaned_output, 'w') as outfile:\n",
    "    outfile.write(\"Family_index\\tMembers\\n\")\n",
    "    for index_f, family in tqdm(enumerate(families_set)):\n",
    "        cluster_list = \",\".join(list(family))\n",
    "        outfile.write(f\"Family_{index_f}\\t{cluster_list}\\n\")\n",
    "\n",
    "# Handle any \"loner\" prophages that didn't cluster\n",
    "loners = []\n",
    "for phage in tqdm(os.listdir(path_phages)):\n",
    "    for family in families_set:\n",
    "        if phage in family:\n",
    "            break\n",
    "    else:\n",
    "        loners.append(phage)\n",
    "\n",
    "# Append loners to the final output\n",
    "with open(cleaned_output, 'a') as outfile:\n",
    "    for loner in loners:\n",
    "        outfile.write(f\"Loner\\t{loner}\\n\")\n",
    "\n",
    "# Integrity check\n",
    "cluster_df = pd.read_csv(cleaned_output, sep=\"\\t\")\n",
    "all_phages = []\n",
    "for row in cluster_df.itertuples():\n",
    "    all_phages.extend(row.Members.split(\",\"))\n",
    "\n",
    "loners_df = cluster_df[cluster_df[\"Family_index\"] == \"Loner\"]\n",
    "family_df = cluster_df[cluster_df[\"Family_index\"] != \"Loner\"]\n",
    "\n",
    "# Final cleanup and consistency check\n",
    "final_output = f\"{path_fastani}/clusters_99_80.extra_clean.2004.v2.tsv\"\n",
    "with open(final_output, 'w') as outfile:\n",
    "    outfile.write(\"prophage_id\\tprophage\\n\")\n",
    "    for idx, row in tqdm(cluster_df.iterrows()):\n",
    "        for member in row[\"Members\"].split(\",\"):\n",
    "            outfile.write(f\"prophage_{idx}\\t{member}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc394a3c-d4b7-454b-ba3a-397fda10d732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_geometric]",
   "language": "python",
   "name": "conda-env-torch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
