{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fe21f-3162-463c-8623-54eae001e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import (accuracy_score, f1_score, matthews_corrcoef,\n",
    "                             precision_score, recall_score, roc_auc_score)\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, label_binarize\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.data import DataLoader, HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import SAGEConv, HeteroConv, to_hetero\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from tqdm import tqdm\n",
    "\n",
    "import TropiGAT_graph\n",
    "import TropiGAT_models\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Constants\n",
    "# **************************************************\n",
    "ultrafiltration = False\n",
    "# **************************************************\n",
    "\n",
    "PATH_WORK = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "DATE = \"27_11_2024\"\n",
    "if ultrafiltration == False :\n",
    "    ENSEMBLE_PATH = f\"{PATH_WORK}/train_nn/TropiSAGE_ensemble_{DATE}\"\n",
    "    ENSEMBLE_PATH_log = f\"{PATH_WORK}/train_nn/TropiSAGE_ensemble_{DATE}_log\"\n",
    "    OPTUNA_PATH = f\"{PATH_WORK}/train_nn/ensemble_20112024_log_optimized_SAGE\"\n",
    "else :\n",
    "    ENSEMBLE_PATH = f\"{PATH_WORK}/train_nn/TropiSAGE_ensemble_ultraF_{DATE}\"\n",
    "    ENSEMBLE_PATH_log = f\"{PATH_WORK}/train_nn/TropiSAGE_ensemble_ultraF_{DATE}_log\"\n",
    "    OPTUNA_PATH = f\"{PATH_WORK}/train_nn/ensemble_20112024_log_optimized_SAGE_ultraF\"\n",
    "\n",
    "os.makedirs(ENSEMBLE_PATH, exist_ok=True)\n",
    "os.makedirs(ENSEMBLE_PATH_log, exist_ok=True)\n",
    "\n",
    "DICO_OPTUNA = {}    \n",
    "for file in os.listdir(OPTUNA_PATH):\n",
    "    kl_type = file.split(\"_\")[0]\n",
    "    best_parameters = json.load(open(f\"{OPTUNA_PATH}/{file}\").read())\n",
    "    DICO_OPTUNA[kl_type] = best_parameters\n",
    "    \n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess the prophage data.\"\"\"\n",
    "    df_info = pd.read_csv(f\"{PATH_WORK}/train_nn/TropiGATv2.final_df_v2.tsv\", sep=\"\\t\", header=0)\n",
    "    df_info = df_info.drop_duplicates(subset=[\"Protein_name\"])\n",
    "    \n",
    "    df_prophages = df_info.drop_duplicates(subset=[\"Phage\"], keep=\"first\")\n",
    "    dico_prophage_info = {row[\"Phage\"]: {\"prophage_strain\": row[\"prophage_id\"], \"ancestor\": row[\"Infected_ancestor\"]} \n",
    "                          for _, row in df_prophages.iterrows()}\n",
    "    \n",
    "    return df_info, dico_prophage_info\n",
    "\n",
    "def filter_prophages(df_info, dico_prophage_info):\n",
    "    \"\"\"Filter prophages to remove duplicates and ensure diversity.\"\"\"\n",
    "    def get_filtered_prophages(prophage):\n",
    "        combinations = []\n",
    "        to_exclude = set()\n",
    "        to_keep = set()\n",
    "        to_keep.add(prophage)\n",
    "        df_prophage_group = df_info[\n",
    "            (df_info[\"prophage_id\"] == dico_prophage_info[prophage][\"prophage_strain\"]) & \n",
    "            (df_info[\"Infected_ancestor\"] == dico_prophage_info[prophage][\"ancestor\"])\n",
    "        ]\n",
    "        if len(df_prophage_group) == 1:\n",
    "            return df_prophage_group, to_exclude, to_keep\n",
    "        \n",
    "        depo_set = set(df_prophage_group[df_prophage_group[\"Phage\"] == prophage][\"domain_seq\"].values)\n",
    "        for prophage_tmp in df_prophage_group[\"Phage\"].unique():\n",
    "            if prophage_tmp != prophage:\n",
    "                tmp_depo_set = set(df_prophage_group[df_prophage_group[\"Phage\"] == prophage_tmp][\"domain_seq\"].values)\n",
    "                if depo_set == tmp_depo_set:\n",
    "                    to_exclude.add(prophage_tmp)\n",
    "                elif tmp_depo_set not in combinations:\n",
    "                    to_keep.add(prophage_tmp)\n",
    "                    combinations.append(tmp_depo_set)\n",
    "                else:\n",
    "                    to_exclude.add(prophage_tmp)\n",
    "        return df_prophage_group, to_exclude, to_keep\n",
    "\n",
    "    good_prophages = set()\n",
    "    excluded_prophages = set()\n",
    "\n",
    "    for prophage in tqdm(dico_prophage_info.keys()):\n",
    "        if prophage not in excluded_prophages and prophage not in good_prophages:\n",
    "            _, excluded_members, kept_members = get_filtered_prophages(prophage)\n",
    "            good_prophages.update(kept_members)\n",
    "            excluded_prophages.update(excluded_members)\n",
    "\n",
    "    df_info_filtered = df_info[df_info[\"Phage\"].isin(good_prophages)]\n",
    "    df_info_final = df_info_filtered[~df_info_filtered[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "\n",
    "    return df_info_final\n",
    "\n",
    "\n",
    "def ultrafilter_prophages(df_info):\n",
    "    \"\"\"Perform ultra-filtration to remove duplicate prophages within KL types.\"\"\"\n",
    "    duplicate_prophage = []\n",
    "    dico_kltype_duplica = {}\n",
    "\n",
    "    for kltype in df_info[\"KL_type_LCA\"].unique():\n",
    "        df_kl = df_info[df_info[\"KL_type_LCA\"] == kltype][[\"Phage\", \"Protein_name\", \"KL_type_LCA\", \"Infected_ancestor\", \"index\", \"seq\", \"domain_seq\"]]\n",
    "        prophages_tmp_list = df_kl[\"Phage\"].unique().tolist()\n",
    "        set_sets_depo = []\n",
    "        duplicated = {}  \n",
    "        for prophage_tmp in prophages_tmp_list: \n",
    "            set_depo = frozenset(df_kl[df_kl[\"Phage\"] == prophage_tmp][\"domain_seq\"].values)\n",
    "            for past_set in set_sets_depo:\n",
    "                if past_set == set_depo:\n",
    "                    duplicated[past_set] = duplicated.get(past_set, 0) + 1\n",
    "                    duplicate_prophage.append(prophage_tmp)\n",
    "                    break\n",
    "            else:\n",
    "                set_sets_depo.append(set_depo)\n",
    "                duplicated[set_depo] = 1\n",
    "        dico_kltype_duplica[kltype] = duplicated\n",
    "\n",
    "    df_info_ultrafiltered = df_info[~df_info[\"Phage\"].isin(duplicate_prophage)]\n",
    "    return df_info_ultrafiltered\n",
    "\n",
    "\n",
    "def prepare_kltypes(df_info):\n",
    "    \"\"\"Prepare KL types for training.\"\"\"\n",
    "    df_prophages = df_info.drop_duplicates(subset=[\"Phage\"])\n",
    "    dico_prophage_count = dict(Counter(df_prophages[\"KL_type_LCA\"]))\n",
    "    kltypes = [kltype for kltype, count in dico_prophage_count.items() if count >= 10]\n",
    "    return kltypes, dico_prophage_count\n",
    "\n",
    "\n",
    "class TropiGAT_small_sage_module(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels, edge_type = (\"B2\", \"expressed\", \"B1\") ,dropout = 0.2, conv = SAGEConv):\n",
    "        super().__init__()\n",
    "        # GATv2 module :\n",
    "        self.conv = conv((-1,-1), hidden_channels)\n",
    "        self.hetero_conv = HeteroConv({edge_type: self.conv})\n",
    "        # FNN layers : \n",
    "        self.linear_layers = nn.Sequential(nn.Linear(hidden_channels, 1280),\n",
    "                                           nn.BatchNorm1d(1280),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           torch.nn.Dropout(dropout),\n",
    "                                           nn.Linear(1280, 480),\n",
    "                                           nn.BatchNorm1d(480),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           torch.nn.Dropout(dropout),\n",
    "                                           nn.Linear(480 , 1))\n",
    "        \n",
    "    def forward(self, graph_data):\n",
    "        x_B1_dict  = self.hetero_conv(graph_data.x_dict, graph_data.edge_index_dict)\n",
    "        x = self.linear_layers(x_B1_dict[\"B1\"])\n",
    "        return x.view(-1)\n",
    "    \n",
    "\n",
    "def train_graph(kl_type, graph_baseline, dico_prophage_kltype_associated, df_info, dico_prophage_count):\n",
    "    \"\"\"Train the graph neural network for a specific KL type.\"\"\"\n",
    "    for seed in range(1, 6):\n",
    "        if os.path.isfile(f\"{ENSEMBLE_PATH_log}/{kl_type}__{seed}__node_classification.{DATE}.log\") == False :\n",
    "            log_file = f\"{ENSEMBLE_PATH_log}/{kl_type}__{seed}__node_classification.{DATE}.log\"\n",
    "            with open(log_file, \"w\") as log_outfile:\n",
    "                try:\n",
    "                    n_prophage = dico_prophage_count[kl_type]\n",
    "                    graph_data_kltype = TropiGAT_graph.build_graph_masking_v2(\n",
    "                        graph_baseline, dico_prophage_kltype_associated, df_info, \n",
    "                        kl_type, 5, 0.7, 0.2, 0.1, seed=seed\n",
    "                    )\n",
    "                    model = TropiGAT_small_sage_module(\n",
    "                        hidden_channels = 1280, \n",
    "                        dropout = DICO_OPTUNA[kl_type][\"dropout\"], \n",
    "                    )\n",
    "\n",
    "                    model(graph_data_kltype)\n",
    "\n",
    "                    optimizer = torch.optim.AdamW(\n",
    "                        model.parameters(), \n",
    "                        lr = DICO_OPTUNA[kl_type][\"lr\"], \n",
    "                        weight_decay = DICO_OPTUNA[kl_type][\"weight_decay\"]\n",
    "                    )\n",
    "                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, min_lr=1e-6)\n",
    "                    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "                    early_stopping = TropiGAT_models.EarlyStopping(\n",
    "                        patience=100, \n",
    "                        verbose=True, \n",
    "                        path=f\"{ENSEMBLE_PATH}/{kl_type}__{seed}.TropiSAGE.{DATE}.pt\", \n",
    "                        metric='MCC'\n",
    "                    )\n",
    "\n",
    "                    for epoch in range(500):\n",
    "                        train_loss = TropiGAT_models.train(model, graph_data_kltype, optimizer, criterion)\n",
    "                        if epoch % 5 == 0:\n",
    "                            test_loss, metrics = TropiGAT_models.evaluate(\n",
    "                                model, graph_data_kltype, criterion, graph_data_kltype[\"B1\"].test_mask\n",
    "                            )\n",
    "                            log_outfile.write(f'Epoch: {epoch}\\tTrain Loss: {train_loss:.4f}\\t'\n",
    "                                              f'Test Loss: {test_loss:.4f}\\tMCC: {metrics[3]:.4f}\\t'\n",
    "                                              f'AUC: {metrics[5]:.4f}\\tAccuracy: {metrics[4]:.4f}\\n')\n",
    "                            scheduler.step(test_loss)\n",
    "                            early_stopping(metrics[3], model, epoch)\n",
    "                            if early_stopping.early_stop:\n",
    "                                log_outfile.write(f\"Early stopping at epoch {epoch}\\n\")\n",
    "                                break\n",
    "                    else:\n",
    "                        torch.save(model, f\"{ENSEMBLE_PATH}/{kl_type}__{seed}.TropiSAGE.{DATE}.pt\")\n",
    "\n",
    "                    # Final evaluation\n",
    "                    model_final = TropiGAT_small_sage_module(\n",
    "                        hidden_channels = 1280, \n",
    "                        dropout = DICO_OPTUNA[kl_type][\"dropout\"], \n",
    "                    )\n",
    "                    model_final.load_state_dict(torch.load(f\"{ENSEMBLE_PATH}/{kl_type}__{seed}.TropiSAGE.{DATE}.pt\"))\n",
    "                    eval_loss, metrics = TropiGAT_models.evaluate(\n",
    "                        model_final, graph_data_kltype, criterion, graph_data_kltype[\"B1\"].eval_mask\n",
    "                    )\n",
    "\n",
    "                    with open(f\"{ENSEMBLE_PATH_log}/Metric_Report.{DATE}.tsv\", \"a+\") as metric_outfile:\n",
    "                        metric_outfile.write(f\"{kl_type}__{seed}\\t{n_prophage}\\t\"\n",
    "                                             f\"{metrics[0]:.4f}\\t{metrics[1]:.4f}\\t{metrics[2]:.4f}\\t\"\n",
    "                                             f\"{metrics[3]:.4f}\\t{metrics[4]:.4f}\\t{metrics[5]:.4f}\\n\")\n",
    "\n",
    "                    log_outfile.write(f\"Final evaluation:\\n\"\n",
    "                                      f\"F1 Score: {metrics[0]:.4f}, Precision: {metrics[1]:.4f}, \"\n",
    "                                      f\"Recall: {metrics[2]:.4f}, MCC: {metrics[3]:.4f}, \"\n",
    "                                      f\"Accuracy: {metrics[4]:.4f}, AUC: {metrics[5]:.4f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    log_outfile.write(f\"Error occurred: {str(e)}\")\n",
    "                    with open(f\"{ENSEMBLE_PATH_log}/Metric_Report.{DATE}.tsv\", \"a+\") as metric_outfile:\n",
    "                        metric_outfile.write(f\"{kl_type}__{seed}\\t{n_prophage}\\t***Error***\\n\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the TropiSAGE workflow.\"\"\"\n",
    "    df_info, dico_prophage_info = load_and_preprocess_data()\n",
    "    if ultrafiltration == True :\n",
    "        df_info_final = ultrafilter_prophages(filter_prophages(df_info, dico_prophage_info))\n",
    "    else :\n",
    "        df_info_final = filter_prophages(df_info, dico_prophage_info)\n",
    "    \n",
    "    kltypes, dico_prophage_count = prepare_kltypes(df_info_filtered)\n",
    "    graph_baseline, dico_prophage_kltype_associated = TropiGAT_graph.build_graph_baseline(df_info_final)\n",
    "    \n",
    "    with ThreadPool(5) as p:\n",
    "        p.starmap(train_graph, [(kl_type, graph_baseline, dico_prophage_kltype_associated, df_info_final, dico_prophage_count) for kl_type in kltypes])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_geometric]",
   "language": "python",
   "name": "conda-env-torch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
