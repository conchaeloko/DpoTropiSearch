{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a6529-2045-4c6a-9fb8-f4307044a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import (accuracy_score, f1_score, matthews_corrcoef,\n",
    "                             precision_score, recall_score, roc_auc_score)\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.nn import SAGEConv, HeteroConv\n",
    "from tqdm import tqdm\n",
    "\n",
    "import TropiGAT_graph\n",
    "import TropiGAT_models\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=243):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Constants\n",
    "PATH_WORK = \"/home/conchae/prediction_depolymerase_tropism/prophage_prediction/depolymerase_decipher/ficheros_28032023\"\n",
    "DATE = \"2011\"\n",
    "ultrafiltration = True\n",
    "\n",
    "if ultrafiltration :\n",
    "    ENSEMBLE_PATH = f\"{PATH_WORK}/train_nn/ensemble_{DATE}2024_optimized_SAGE_ultraF\"\n",
    "    ENSEMBLE_PATH_log = f\"{PATH_WORK}/train_nn/ensemble_{DATE}2024_log_optimized_SAGE_ultraF\"\n",
    "else :\n",
    "    ENSEMBLE_PATH = f\"{PATH_WORK}/train_nn/ensemble_{DATE}2024_optimized_SAGE\"\n",
    "    ENSEMBLE_PATH_log = f\"{PATH_WORK}/train_nn/ensemble_{DATE}2024_log_optimized_SAGE\"\n",
    "\n",
    "os.makedirs(ENSEMBLE_PATH, exist_ok=True)\n",
    "os.makedirs(ENSEMBLE_PATH_log, exist_ok=True)\n",
    "\n",
    "# Existing functions from your original code\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess the prophage data.\"\"\"\n",
    "    df_info = pd.read_csv(f\"{PATH_WORK}/train_nn/TropiGATv2.final_df_v2.tsv\", sep=\"\\t\", header=0)\n",
    "    df_info = df_info.drop_duplicates(subset=[\"Protein_name\"])\n",
    "    \n",
    "    df_prophages = df_info.drop_duplicates(subset=[\"Phage\"], keep=\"first\")\n",
    "    dico_prophage_info = {row[\"Phage\"]: {\"prophage_strain\": row[\"prophage_id\"], \"ancestor\": row[\"Infected_ancestor\"]} \n",
    "                          for _, row in df_prophages.iterrows()}\n",
    "    \n",
    "    return df_info, dico_prophage_info\n",
    "\n",
    "def filter_prophages(df_info, dico_prophage_info):\n",
    "    \"\"\"Filter prophages to remove duplicates and ensure diversity.\"\"\"\n",
    "    def get_filtered_prophages(prophage):\n",
    "        combinations = []\n",
    "        to_exclude = set()\n",
    "        to_keep = set()\n",
    "        to_keep.add(prophage)\n",
    "        df_prophage_group = df_info[\n",
    "            (df_info[\"prophage_id\"] == dico_prophage_info[prophage][\"prophage_strain\"]) & \n",
    "            (df_info[\"Infected_ancestor\"] == dico_prophage_info[prophage][\"ancestor\"])\n",
    "        ]\n",
    "        if len(df_prophage_group) == 1:\n",
    "            return df_prophage_group, to_exclude, to_keep\n",
    "        \n",
    "        depo_set = set(df_prophage_group[df_prophage_group[\"Phage\"] == prophage][\"domain_seq\"].values)\n",
    "        for prophage_tmp in df_prophage_group[\"Phage\"].unique():\n",
    "            if prophage_tmp != prophage:\n",
    "                tmp_depo_set = set(df_prophage_group[df_prophage_group[\"Phage\"] == prophage_tmp][\"domain_seq\"].values)\n",
    "                if depo_set == tmp_depo_set:\n",
    "                    to_exclude.add(prophage_tmp)\n",
    "                elif tmp_depo_set not in combinations:\n",
    "                    to_keep.add(prophage_tmp)\n",
    "                    combinations.append(tmp_depo_set)\n",
    "                else:\n",
    "                    to_exclude.add(prophage_tmp)\n",
    "        return df_prophage_group, to_exclude, to_keep\n",
    "\n",
    "    good_prophages = set()\n",
    "    excluded_prophages = set()\n",
    "\n",
    "    for prophage in tqdm(dico_prophage_info.keys()):\n",
    "        if prophage not in excluded_prophages and prophage not in good_prophages:\n",
    "            _, excluded_members, kept_members = get_filtered_prophages(prophage)\n",
    "            good_prophages.update(kept_members)\n",
    "            excluded_prophages.update(excluded_members)\n",
    "\n",
    "    df_info_filtered = df_info[df_info[\"Phage\"].isin(good_prophages)]\n",
    "    df_info_final = df_info_filtered[~df_info_filtered[\"KL_type_LCA\"].str.contains(\"\\\\|\")]\n",
    "\n",
    "    return df_info_final\n",
    "\n",
    "\n",
    "\n",
    "def ultrafilter_prophages(df_info):\n",
    "    \"\"\"Perform ultra-filtration to remove duplicate prophages within KL types.\"\"\"\n",
    "    duplicate_prophage = []\n",
    "    dico_kltype_duplica = {}\n",
    "\n",
    "    for kltype in df_info[\"KL_type_LCA\"].unique():\n",
    "        df_kl = df_info[df_info[\"KL_type_LCA\"] == kltype][[\"Phage\", \"Protein_name\", \"KL_type_LCA\", \"Infected_ancestor\", \"index\", \"seq\", \"domain_seq\"]]\n",
    "        prophages_tmp_list = df_kl[\"Phage\"].unique().tolist()\n",
    "        set_sets_depo = []\n",
    "        duplicated = {}  \n",
    "        for prophage_tmp in prophages_tmp_list: \n",
    "            set_depo = frozenset(df_kl[df_kl[\"Phage\"] == prophage_tmp][\"domain_seq\"].values)\n",
    "            for past_set in set_sets_depo:\n",
    "                if past_set == set_depo:\n",
    "                    duplicated[past_set] = duplicated.get(past_set, 0) + 1\n",
    "                    duplicate_prophage.append(prophage_tmp)\n",
    "                    break\n",
    "            else:\n",
    "                set_sets_depo.append(set_depo)\n",
    "                duplicated[set_depo] = 1\n",
    "        dico_kltype_duplica[kltype] = duplicated\n",
    "\n",
    "    df_info_ultrafiltered = df_info[~df_info[\"Phage\"].isin(duplicate_prophage)]\n",
    "    \n",
    "    if ultrafiltration :\n",
    "        return df_info_ultrafiltered\n",
    "    else :\n",
    "        return df_info\n",
    "\n",
    "\n",
    "\n",
    "def prepare_kltypes(df_info):\n",
    "    \"\"\"Prepare KL types for training.\"\"\"\n",
    "    df_prophages = df_info.drop_duplicates(subset=[\"Phage\"])\n",
    "    dico_prophage_count = dict(Counter(df_prophages[\"KL_type_LCA\"]))\n",
    "    kltypes = [kltype for kltype, count in dico_prophage_count.items() if count >= 10]\n",
    "    return kltypes, dico_prophage_count\n",
    "\n",
    "\n",
    "class TropiGAT_small_sage_module(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels, edge_type = (\"B2\", \"expressed\", \"B1\") ,dropout = 0.2, conv = SAGEConv):\n",
    "        super().__init__()\n",
    "        # GATv2 module :\n",
    "        self.conv = conv((-1,-1), hidden_channels)\n",
    "        self.hetero_conv = HeteroConv({edge_type: self.conv})\n",
    "        # FNN layers : \n",
    "        self.linear_layers = nn.Sequential(nn.Linear(hidden_channels, 1280),\n",
    "                                           nn.BatchNorm1d(1280),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           torch.nn.Dropout(dropout),\n",
    "                                           nn.Linear(1280, 480),\n",
    "                                           nn.BatchNorm1d(480),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           torch.nn.Dropout(dropout),\n",
    "                                           nn.Linear(480 , 1))\n",
    "        \n",
    "    def forward(self, graph_data):\n",
    "        x_B1_dict  = self.hetero_conv(graph_data.x_dict, graph_data.edge_index_dict)\n",
    "        x = self.linear_layers(x_B1_dict[\"B1\"])\n",
    "        return x.view(-1)\n",
    "    \n",
    "\n",
    "def log_trial_hyperparameters(trial_number, hyperparameters, log_file):\n",
    "    \"\"\"Log the hyperparameters for a specific trial.\"\"\"\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"Trial {trial_number} Hyperparameters: {json.dumps(hyperparameters)}\\n\")\n",
    "\n",
    "def log_trial_loss(trial_number, epoch, train_loss, test_loss, log_file):\n",
    "    \"\"\"Log the loss for each trial and epoch to a file.\"\"\"\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f\"Trial {trial_number}, Epoch {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\\n\")\n",
    "\n",
    "def objective(trial, graph_baseline, dico_prophage_kltype_associated, df_info, kl_type, dico_prophage_count, log_file):\n",
    "    set_seed(243)  # Ensure reproducibility\n",
    "    \n",
    "    # Hyperparameters to optimize\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-7, 1e-3)\n",
    "    dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n",
    "    \n",
    "    # Log hyperparameters at the start of the trial\n",
    "    hyperparameters = {'lr': lr, 'weight_decay': weight_decay, 'dropout': dropout}\n",
    "    log_trial_hyperparameters(trial.number, hyperparameters, log_file)\n",
    "    \n",
    "    # Build graph for this KL type\n",
    "    graph_data_kltype = TropiGAT_graph.build_graph_masking_v2(\n",
    "        graph_baseline, dico_prophage_kltype_associated, df_info, \n",
    "        kl_type, 5, 0.7, 0.2, 0.1, seed=243\n",
    "    )\n",
    "    \n",
    "    # Initialize model with suggested hyperparameters\n",
    "    model = TropiGAT_small_sage_module(\n",
    "        hidden_channels=1280, \n",
    "        dropout=dropout, \n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=lr, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(200):  # Reduced epochs for faster trials\n",
    "        train_loss = TropiGAT_models.train(model, graph_data_kltype, optimizer, criterion)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            test_loss, metrics = TropiGAT_models.evaluate(\n",
    "                model, graph_data_kltype, criterion, graph_data_kltype[\"B1\"].test_mask\n",
    "            )\n",
    "            scheduler.step(test_loss)\n",
    "            # Log the loss for the current trial and epoch\n",
    "            log_trial_loss(trial.number, epoch, train_loss, test_loss, log_file)\n",
    "            # Track best loss\n",
    "            best_loss = min(best_loss, test_loss)\n",
    "            # Optuna pruning\n",
    "            trial.report(best_loss, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "    \n",
    "    return best_loss\n",
    "\n",
    "def train_optimized_graph(kl_type, graph_baseline, dico_prophage_kltype_associated, df_info, dico_prophage_count):\n",
    "    # Create a study object and optimize the objective function.\n",
    "    if os.path.isfile(f\"{ENSEMBLE_PATH_log}/{kl_type}_optuna_best_params.json\") == False :\n",
    "        study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "        # Path to the loss log file\n",
    "        log_file = f\"{ENSEMBLE_PATH_log}/{kl_type}_loss_log.txt\"\n",
    "        study.optimize(\n",
    "            lambda trial: objective(\n",
    "                trial, graph_baseline, dico_prophage_kltype_associated, \n",
    "                df_info, kl_type, dico_prophage_count, log_file\n",
    "            ), \n",
    "            n_trials=100\n",
    "        )\n",
    "        # Log best hyperparameters\n",
    "        best_params = study.best_params\n",
    "        best_params['kl_type'] = kl_type    \n",
    "        with open(f\"{ENSEMBLE_PATH_log}/{kl_type}_optuna_best_params.json\", 'w') as f:\n",
    "            json.dump(best_params, f)\n",
    "\n",
    "        return study.best_params\n",
    "    else :\n",
    "        pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the TropiGAT workflow.\"\"\"\n",
    "    set_seed(243)\n",
    "    df_info, dico_prophage_info = load_and_preprocess_data()\n",
    "    df_info_filtered = filter_prophages(df_info, dico_prophage_info)\n",
    "    df_info_final = ultrafilter_prophages(df_info_filtered)\n",
    "    \n",
    "    kltypes, dico_prophage_count = prepare_kltypes(df_info_final)\n",
    "    \n",
    "    graph_baseline, dico_prophage_kltype_associated = TropiGAT_graph.build_graph_baseline(df_info_final)\n",
    "    \n",
    "    # Parallel optimization of hyperparameters for each KL type\n",
    "    with ThreadPool(5) as p:\n",
    "        best_params_list = p.starmap(\n",
    "            train_optimized_graph, \n",
    "            [(kl_type, graph_baseline, dico_prophage_kltype_associated, df_info_final, dico_prophage_count) for kl_type in kltypes]\n",
    "        )\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_geometric]",
   "language": "python",
   "name": "conda-env-torch_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
